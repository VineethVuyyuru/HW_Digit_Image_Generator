{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NjJ94ofOWMJ"
   },
   "source": [
    "## Conditional WGAN for generating handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4zvZ_mvOc65"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Krzyy6uIORdC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUsbiuoPPy5D"
   },
   "source": [
    "### Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_mGkUDozP1ik"
   },
   "outputs": [],
   "source": [
    "# Clip weights to a given hypercube\n",
    "class ClipConstrainer(keras.constraints.Constraint):\n",
    "  def __init__(self, clip_value):\n",
    "    self.clip_value = clip_value\n",
    "\n",
    "  def __call__(self, weights):\n",
    "    return backend.clip(weights, self.clip_value * -1, self.clip_value)\n",
    "\n",
    "  def get_config(self):\n",
    "    return {'clip_value' : self.clip_value}\n",
    "\n",
    "#Custom loss function to calculate wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "  return backend.mean(y_true * y_pred)\n",
    "\n",
    "def define_critic(image_shape = (28,28,1), n_classes = 10):\n",
    "  in_label = Input(shape=(1,))\n",
    "  li = Embedding(n_classes, 50)(in_label)\n",
    "  n_nodes = image_shape[0]*image_shape[1]\n",
    "  li = Dense(n_nodes)(li)\n",
    "  li = Reshape((image_shape[0], image_shape[1], 1))(li)\n",
    "  in_image = Input(shape = image_shape)\n",
    "  merge = Concatenate()([in_image, li])\n",
    "\n",
    "  weight_init = keras.initializers.RandomNormal(stddev = 0.02)\n",
    "  weight_constrain = ClipConstrainer(0.01)\n",
    "\n",
    "  model = Conv2D(64, (4,4), (2,2), padding=\"same\", kernel_initializer=weight_init, kernel_constraint=weight_constrain, input_shape=image_shape)(merge)\n",
    "  model = LeakyReLU(0.2)(model)\n",
    "  model = BatchNormalization()(model)\n",
    "\n",
    "  model = Conv2D(64, (4,4), (2,2), padding=\"same\", kernel_initializer=weight_init, kernel_constraint=weight_constrain)(model)\n",
    "  model = LeakyReLU(0.2)(model)\n",
    "  model = BatchNormalization()(model)\n",
    "\n",
    "  model = Flatten()(model)\n",
    "  model = Dropout(0.4)(model)\n",
    "  model = Dense(1)(model)\n",
    "    \n",
    "  critic = Model([in_image, in_label], model)\n",
    "  opt = RMSprop(learning_rate=0.00005)\n",
    "  critic.compile(optimizer=opt, loss=wasserstein_loss, metrics=['accuracy'])\n",
    "  return critic\n",
    "\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "  in_label = Input(shape = (1,))\n",
    "  li = Embedding(n_classes, 50)(in_label)\n",
    "  weight_init = keras.initializers.RandomNormal(stddev= 0.02)\n",
    "  num_nodes = 7 * 7\n",
    "  li = Dense(num_nodes)(li)\n",
    "  li = Reshape((7,7,1))(li)\n",
    "  in_lat = Input(shape = (latent_dim,))\n",
    "  num_nodes = 7 * 7 * 128\n",
    "  model = Dense(num_nodes, kernel_initializer=weight_init)(in_lat)\n",
    "  model = LeakyReLU(0.2)(model)\n",
    "  model = Reshape((7,7,128))(model)\n",
    "  merge = Concatenate()([model, li])\n",
    "\n",
    "  model = Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=weight_init)(merge)\n",
    "  model = BatchNormalization()(model)\n",
    "  model = LeakyReLU(0.2)(model)\n",
    "\n",
    "  model = Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=weight_init)(model)\n",
    "  model = BatchNormalization()(model)\n",
    "  model = LeakyReLU(0.2)(model)\n",
    "\n",
    "  out_layer = Conv2D(1, (7,7), activation=\"tanh\", padding=\"same\", kernel_initializer=weight_init)(model)\n",
    "  gen = Model([in_lat, in_label], out_layer)\n",
    "  return gen\n",
    "\n",
    "def define_gan(critic, generator):\n",
    "  critic.trainable = False\n",
    "  gen_noise, gen_label = generator.input\n",
    "  gen_output = generator.output\n",
    "  gan_output = critic([gen_output, gen_label])\n",
    "  model = Model([gen_noise, gen_label], gan_output)\n",
    "  opt= RMSprop(learning_rate=0.00005)\n",
    "  model.compile(optimizer=opt, loss=wasserstein_loss)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7LBTccafLjS"
   },
   "source": [
    "### Helper functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4FckV9iwfPDI"
   },
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "  (train_x, train_y), (_,_) = keras.datasets.mnist.load_data()\n",
    "  X = np.expand_dims(train_x, axis=-1)\n",
    "  X = X.astype('float32')\n",
    "  X = (X - 127.5) / 127.5\n",
    "  return [X, train_y]\n",
    "\n",
    "def generate_real_samples(dataset, num_samples):\n",
    "  images, labels = dataset\n",
    "  indices = np.random.randint(0, images.shape[0], num_samples)\n",
    "  X, labels = images[indices], labels[indices]\n",
    "  y = np.ones(num_samples) * -1\n",
    "  return [X, labels], y\n",
    "\n",
    "def generate_latent_points(latent_dim, num_samples, n_classes = 10):\n",
    "  x_input = np.random.randn(latent_dim * num_samples)\n",
    "  z_input = x_input.reshape(num_samples, latent_dim)\n",
    "  labels = np.random.randint(0, n_classes, num_samples)\n",
    "  return [z_input, labels]\n",
    "\n",
    "def generate_fake_samples(gen, latent_dim, num_samples):\n",
    "  z_input, label_input = generate_latent_points(latent_dim, num_samples)\n",
    "  images = gen.predict([z_input, label_input])\n",
    "  y = np.ones(num_samples)\n",
    "  return [images, label_input], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDwTQNbkkrYE"
   },
   "source": [
    "### Define the train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wrzxZlAMktUl"
   },
   "outputs": [],
   "source": [
    "from numpy.ma.core import mean\n",
    "def summarize_performance(step, gen,critic, latent_dim):\n",
    "  num_samples = 100\n",
    "  [X, labels], _ = generate_fake_samples(gen, latent_dim, num_samples)\n",
    "  X = (X + 1)/2\n",
    "\n",
    "  #Generating images \n",
    "  for i in range(10*10):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X[i, :, :, 0], cmap=\"gray\")\n",
    "  plot_file_name = \"generated_images_%04d.png\" % (step + 1)\n",
    "  plt.savefig('c_eval\\\\' + plot_file_name)\n",
    "  plt.close()\n",
    "\n",
    "  #Saving generator weights\n",
    "  gen_file_name = \"gen_%04d.h5\" % (step + 1)\n",
    "  gen.save('c_model\\\\' + gen_file_name)\n",
    "  print(f\">Saved {plot_file_name} and {gen_file_name}.\")\n",
    "\n",
    "  #Saving critic weights\n",
    "  critic_file_name = \"critic_%04d.h5\" % (step + 1)\n",
    "  gen.save('c_model\\\\' + critic_file_name)\n",
    "  print(f\">Saved {plot_file_name} and, critic and gen models.\")\n",
    "\n",
    "def plot_history(c_real_hist, c_fake_hist, gan_hist):\n",
    "  plt.plot(c_real_hist, label=\"critic_real\")\n",
    "  plt.plot(c_fake_hist, label=\"critic_fake\")\n",
    "  plt.plot(gan_hist, label=\"GAN\")\n",
    "  plt.legend()\n",
    "  plt.savefig(\"loss_history.png\")\n",
    "  plt.close()\n",
    "\n",
    "def train_gan(gen, critic, gan, dataset, latent_dim, batch_size = 64, num_epochs = 20, num_critic = 5):\n",
    "  batches_per_epoch = int(dataset[0].shape[0] / batch_size)\n",
    "  c_real_hist, c_fake_hist, g_hist = list(), list(), list()\n",
    "  for i in range(num_epochs):\n",
    "    for j in range(batches_per_epoch):\n",
    "        cr_tmp, cf_tmp = list(), list()\n",
    "        for _ in range(num_critic):\n",
    "          [X_real, labels_real], y_real = generate_real_samples(dataset, int(batch_size/2))\n",
    "          c_loss_real = critic.train_on_batch([X_real, labels_real], y_real)\n",
    "          cr_tmp.append(c_loss_real)\n",
    "          [X_fake, labels_fake], y_fake = generate_fake_samples(gen, latent_dim, int(batch_size/2))\n",
    "          c_loss_fake = critic.train_on_batch([X_fake, labels_fake], y_fake)\n",
    "          cf_tmp.append(c_loss_fake)\n",
    "        c_real_hist.append(mean(cr_tmp))\n",
    "        c_fake_hist.append(mean(cf_tmp))\n",
    "        [X_gan, labels] = generate_latent_points(latent_dim, batch_size)\n",
    "        y_gan = -1 * np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch([X_gan, labels], y_gan)\n",
    "        g_hist.append(g_loss)\n",
    "    print(f\"Epoch: {i+1}/{num_epochs} c_real:{c_real_hist[-1]} c_fake:{c_fake_hist[-1]} GAN:{g_hist[-1]}\")\n",
    "    summarize_performance(i, gen,critic, latent_dim)\n",
    "  plot_history(c_real_hist, c_fake_hist, g_hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5WbltVu3OV"
   },
   "source": [
    "### Initialize models and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aGMZpn4u7C0"
   },
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "critic = define_critic()\n",
    "critic.summary()\n",
    "generator = define_generator(latent_dim)\n",
    "generator.summary()\n",
    "gan = define_gan(critic, generator)\n",
    "dataset = load_real_samples()\n",
    "train_gan(generator, critic, gan, dataset, latent_dim)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
